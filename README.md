# AlphaCare Insurance Solutions - Risk Analytics & Predictive Modeling

## Objective
his repository contains the code, analysis, and reports for the AlphaCare Insurance Risk Analytics & Predictive Modeling project. The goal is to analyze historical car insurance claim data from South Africa (February 2014 to August 2015) to identify low-risk customer segments, optimize premiums, and support marketing strategies. This helps AlphaCare Insurance Solutions (ACIS) attract new clients by reducing premiums for low-risk targets.



## Project Structure
      alphacare-insurance-analytics/
      â”œâ”€â”€ data/
      â”‚   â”œâ”€â”€ raw/                  # Raw dataset (insurance_data_feb2014_aug2015.xlsx)
      â”‚   â””â”€â”€ processed/            # Cleaned/processed data files
      â”œâ”€â”€ notebooks/
      â”‚     â””â”€â”€ 01_eda_exploration.ipynb  # Jupyter notebook for EDA and initial stats
      â”œâ”€â”€ src/
      â”‚   â”œâ”€â”€ data_loader.py        # Data loading utilities
      â”‚   â”œâ”€â”€ eda.py                # EDA functions
      â”‚   â”œâ”€â”€ hypothesis_testing.py # Hypothesis testing scripts
      â”‚   â”œâ”€â”€ modeling.py           # Predictive modeling code
      â”‚   â””â”€â”€ utils.py              # Helper utilities
      â”œâ”€â”€ reports/
      â”‚   â””â”€â”€ eda_report.pdf        # Summary report of EDA and statistical findings
      â”œâ”€â”€ output/
      â”‚   â”œâ”€â”€ loss_ratio_by_province.png
      â”‚   â”œâ”€â”€ claims_vs_premium_gender.png
      â”‚   â”œâ”€â”€ top_risky_makes.png
      â”‚   â””â”€â”€ feature_importance.png
      â”œâ”€â”€ tests/                    # Unit tests (e.g., for data loading and models)
      â”œâ”€â”€ requirements.txt          # Python dependencies
      â”œâ”€â”€ README.md                 # This file
      â””â”€â”€ .github/workflows/ci.yml  # CI/CD pipeline for GitHub Actions

### ğŸš€ Setup and Installation
  1. Clone the Repository
       git clone https://github.com/yourusername/alphacare-insurance-analytics.git
       cd alphacare-insurance-analytics

  2. Create and Activate a Virtual Environment
     
      Windows
      python -m venv venv
      venv\Scripts\activate

 3. Install Dependencies
      pip install -r requirements.txt

 4. Add the Raw Data

      Download the raw dataset and place it in:
      data/raw/

 5. Run the EDA Notebook
       jupyter notebook notebooks/01_eda_exploration.ipynb

### ğŸ“˜ Task 1 â€” Git, GitHub, EDA & Statistical Analysis

 Task 1 establishes the foundation of the project, including environment setup, version control practices, exploratory analysis, and statistical testing.
 A dedicated branch was used:

       task-1-eda-stats


#### ğŸ” Key Activities
 1. Data Understanding & EDA

 - Loaded and explored the dataset.
 - Performed descriptive statistics, missing-value checks, and outlier detection.
 - Conducted univariate and bivariate analysis.
 - Explored relationships across regions, vehicle classes, and demographics.
 - Created visual insights saved in output/.

 2. Statistical Analysis
 - Computed loss ratios.
 - Used boxplots and histograms for risk distribution analysis.
 - Performed hypothesis tests on risk differences.

 3. Visualizations (3 Highlighted Plots)
 - Loss Ratio by Province
 - Claims vs Premium by Gender (log scale)
 - Top 10 Riskiest Vehicle Makes

 4. Initial Modeling

 - Built a simple Random Forest model.
 - Extracted feature importances to understand premium drivers.

----
```markdown
# ğŸ§© Task 2 â€” Reproducible Data Pipeline with DVC

In regulated industriesâ€”such as insurance, banking, lending, and healthcareâ€”every dataset used to produce a model or analysis must be traceable and reproducible. DVC (Data Version Control) ensures datasets are version-controlled with the same rigor as code.

## âœ… 1. Install DVC
```bash
pip install dvc
```

### Initialize DVC in the project root:
```bash
dvc init
```

## âœ… 2. Configure Local Remote Storage

Create a directory for DVC-managed storage:
```bash
mkdir /path/to/local/storage
```

Add it as your default remote:
```bash
dvc remote add -d localstorage /path/to/local/storage
```

## âœ… 3. Add Data Files to DVC

Place raw datasets into the `data/raw/` directory and track them:
```bash
dvc add data/raw/portfolio.csv
dvc add data/raw/claims.csv
```

This generates `.dvc` pointer files. Commit them:
```bash
git add data/raw/*.dvc .gitignore
git commit -m "Tracked datasets with DVC"
```

## âœ… 4. Push Data to Remote
```bash
dvc push
```

Your dataset is now fully versioned and reproducible.


---

# ğŸ§ª Task 3 â€” Statistical Hypothesis Testing & Risk Segmentation

This task evaluates whether features such as province, postal code, gender, and vehicle make drive meaningful risk differences.

## ğŸ¯ Risk Metrics Used
- **Claim Frequency**: Fraction of policies with â‰¥ 1 claim.
- **Claim Severity**: Average cost of a claim (conditional on claiming).
- **Margin**: TotalPremium âˆ’ TotalClaims.

## ğŸ¯ Null Hypotheses Evaluated
| Hypothesis | Feature         | Objective                                        |
|------------|-----------------|--------------------------------------------------|
| Hâ‚€â‚       | Province        | No risk differences across provinces               |
| Hâ‚€â‚‚       | Postal Code     | No risk differences between zip codes             |
| Hâ‚€â‚ƒ       | Margin          | No profit difference across zip codes             |
| Hâ‚€â‚„       | Gender          | No risk difference between genders                |

## ğŸ§­ Steps Followed
1ï¸âƒ£ **Select KPIs**
   - Claim Frequency
   - Claim Severity
   - Margin (profitability)

2ï¸âƒ£ **Build A/B Groups**
   - Group A = Control
   - Group B = Test
   - Validated group equivalence using Standardized Mean Difference (SMD < 0.1)

3ï¸âƒ£ **Statistical Tests**
   - Chi-square â†’ Frequency differences
   - Mannâ€“Whitney / Kruskalâ€“Wallis â†’ Severity
   - Z-test for Proportions
   - Bootstrap Confidence Intervals
   - Bonferroni-corrected pairwise comparisons

4ï¸âƒ£ **Report & Interpret**
   Results fully documented below.

## ğŸ“Š Full Risk & Profitability Analysis Report (Task 3)

### 1. Overview
This analysis evaluates claim frequency, severity, and overall profitability across multiple customer segments.

**Portfolio-level Metrics**
- Overall Claim Frequency: 0.00279
- Average Severity: 23,273.39
- Total Margin: â€“2,955,983 (negative â†’ portfolio underpriced)

### 2. Provincial Differences (Hâ‚€â‚)
**Result**: Null rejected for both frequency and severity.
- **Claim Frequency (High â†’ Low)**: Gauteng, KwaZulu-Natal, ... Northern Cape, Free State
- **Claim Severity (High â†’ Low)**: Free State, KwaZulu-Natal, ... Northern Cape, Limpopo

**Conclusion**: Provinces must be included in the pricing model.

### 3. Zip Code Differences (Hâ‚€â‚‚ & Hâ‚€â‚ƒ)
**Result**: Strong rejection for both frequency and margin.
- Frequency Chi-square: p < 0.000001
- Margin Kruskalâ€“Wallis: p < 0.000001

**Conclusion**: Postal code = strongest geographical predictor of insurance risk.

### 4. Gender Differences (Hâ‚€â‚„)
**Result**: Fail to reject.
- Frequency: p = 0.951
- Severity: p = 0.223

**Conclusion**: Gender does not influence risk â†’ remove from pricing.

### 5. Vehicle Make & Severity
- Large variation observed across makes.
- Gender effect negligible.
- Some makes show extreme severity (small N).

**Conclusion**: Vehicle make is a valid pricing factor.

### 6. A/B Segmentation Summary
- Groups well balanced (SMD < 0.1).
- Frequency differs: p = 0.013.
- Severity: borderline, but bootstrap confirms real difference.

### 7. Portfolio Recommendations
**Pricing**:
- Add province + postal code relativity factors.
- Adjust vehicle make loadings.

**Underwriting**:
- Investigate high-loss zip codes.
- Analyze severity-heavy regions.

---

# ğŸ¤– Task 4 â€” Predictive Modeling for Risk-Based Pricing

Build claim severity and premium optimization models that form the basis of a pricing engine.

## ğŸ§­ 1. Data Preparation
**Handling Missing Data**:
- Imputed numeric fields.
- Filled categorical NAs with "Unknown".

**Feature Engineering**:
- Vehicle age.
- Claim indicator.
- Log-severity.
- Geography Ã— Vehicle interactions.

**Encoding**:
```python
pd.get_dummies(df, drop_first=True)
```

**Train-Test Split**:
```python
train, test = train_test_split(df, test_size=0.2, random_state=42)
```

## ğŸ§  2. Models Built
ğŸ”¹ **Severity Model (Regression)**:
- Linear Regression.
- Random Forest Regressor.
- XGBoost Regressor.
- Metrics: RMSE, RÂ².

ğŸ”¹ **Claim Probability Model (Classification)**:
- Logistic Regression.
- Random Forest Classifier.
- XGBoost Classifier.
- Metrics: AUC, Recall, Precision, F1.

ğŸ”¹ **Combined Premium Model (Conceptual)**:
- Premium = P(Claim) Ã— Predicted Severity + Expense Loading + Profit Margin.

## ğŸ“Š 3. Model Evaluation
| Model                 | RMSE  | RÂ²   | Notes                        |
|-----------------------|-------|------|------------------------------|
| Linear Regression      | High  | Low  | Baseline only                |
| Random Forest          | Lower | Better | Handles non-linearity      |
| XGBoost                | Best  | Highest | Recommended                |

## ğŸ” 4. Model Interpretability (SHAP)
**Key insights**:
- Older vehicles â†’ significantly higher severity.
- Certain makes â†’ higher predicted cost.
- Province contributes strongly to both severity & claim probability.

**Business Interpretation Example**:
â€œEach additional year of vehicle age increases expected severity by ~X Rand, validating the need for stronger age-based rating factors.â€

## ğŸŸ© Minimum Requirements Completed
- Task 3 merged into main via PR.
- New branch `task-4` created.
- Clean, descriptive commits.
- Full data preparation pipeline.
- Multiple predictive models implemented.
- Evaluation using RMSE, RÂ², AUC.
- SHAP interpretability delivered.

## âœ… Final Summary
This project now includes:
- A fully reproducible dataset pipeline using DVC.
- A statistically validated segmentation of geographical & vehicle risk.
- Predictive models for both claim probability and severity.
- A business-ready premium framework grounded in statistical and actuarial rigor.


```

